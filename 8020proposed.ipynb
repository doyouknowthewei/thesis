{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04ebd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging datasets...\n",
      "Merged dataset shape: (184354, 2)\n",
      "\n",
      "Balancing classes...\n",
      "Balanced dataset shape: (71828, 2)\n",
      "\n",
      "Cleaning text...\n",
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "c:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training Epoch 1:  31%|███▏      | 565/1796 [16:47<36:35,  1.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 330\u001b[0m\n\u001b[0;32m    328\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n\u001b[0;32m    329\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m--> 330\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    333\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 1. Prepare Labels function\n",
    "def prepare_labels(df):\n",
    "    text_col = None\n",
    "    for possible_col in [\"comment_text\", \"text\"]:\n",
    "        if possible_col in df.columns:\n",
    "            text_col = possible_col\n",
    "            break\n",
    "    if text_col is None:\n",
    "        raise ValueError(f\"No recognized text column found. Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    if \"toxic\" in df.columns:\n",
    "        df[\"class\"] = np.where(df[\"toxic\"] >= 0.5, 1, 0)\n",
    "    else:\n",
    "        raise ValueError(f\"No 'toxic' column found for labels. Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    return df[[text_col, \"class\"]].rename(columns={text_col: \"tweet\"})\n",
    "\n",
    "# 2. Load and merge datasets\n",
    "print(\"Loading and merging datasets...\")\n",
    "\n",
    "df1 = pd.read_csv(\"./data/labeled_data.csv\")  # Davidson et al.\n",
    "df1 = df1.rename(columns={\"tweet\": \"tweet\", \"class\": \"label\"})\n",
    "df1[\"class\"] = df1[\"label\"].apply(lambda x: 1 if x in [0, 1] else 0)\n",
    "df1 = df1[[\"tweet\", \"class\"]]\n",
    "\n",
    "df2 = pd.read_csv(\"./data/train.csv\")  # Jigsaw dataset\n",
    "df2_prepared = prepare_labels(df2)\n",
    "\n",
    "combined_df = pd.concat([df1, df2_prepared], ignore_index=True)\n",
    "combined_df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "\n",
    "print(\"Merged dataset shape:\", combined_df.shape)\n",
    "prepared_df = combined_df\n",
    "\n",
    "# 3. Balance dataset\n",
    "print(\"\\nBalancing classes...\")\n",
    "df_majority = prepared_df[prepared_df[\"class\"] == 0]\n",
    "df_minority = prepared_df[prepared_df[\"class\"] == 1]\n",
    "if len(df_minority) == 0:\n",
    "    raise ValueError(\"No toxic samples found in dataset!\")\n",
    "\n",
    "df_majority_downsampled = resample(df_majority, n_samples=len(df_minority), random_state=42)\n",
    "balanced_df = pd.concat([df_majority_downsampled, df_minority]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced dataset shape:\", balanced_df.shape)\n",
    "\n",
    "# 4. Text cleaning utils\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|\\#\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"\\nCleaning text...\")\n",
    "balanced_df['cleaned_text'] = balanced_df['tweet'].apply(clean_text)\n",
    "\n",
    "# 5. Subjectivity and identity term features\n",
    "IDENTITY_TERMS = [\n",
    "    \"muslim\", \"jew\", \"black\", \"white\", \"asian\", \"latino\", \"women\", \"men\", \"gay\", \"lesbian\",\n",
    "    \"trans\", \"christian\", \"atheist\", \"immigrant\", \"indian\", \"arab\", \"african\", \"hispanic\"\n",
    "]\n",
    "\n",
    "def compute_subjectivity(text):\n",
    "    return float(TextBlob(text).sentiment.subjectivity)\n",
    "\n",
    "def has_identity_term(text):\n",
    "    text_lower = text.lower()\n",
    "    return int(any(term in text_lower for term in IDENTITY_TERMS))\n",
    "\n",
    "balanced_df['subjectivity'] = balanced_df['tweet'].apply(compute_subjectivity)\n",
    "balanced_df['identity_flag'] = balanced_df['tweet'].apply(has_identity_term)\n",
    "\n",
    "# 6. Tokenizer setup\n",
    "MAX_LEN = 128\n",
    "VOCAB_SIZE = 30000  # Consistent with your embedding layer\n",
    "\n",
    "# LSTM tokenizer (Keras style)\n",
    "lstm_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "lstm_tokenizer.fit_on_texts(balanced_df['cleaned_text'].tolist())\n",
    "import pickle\n",
    "with open('lstm_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(lstm_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "word_index = lstm_tokenizer.word_index\n",
    "\n",
    "def lstm_tokenize(texts):\n",
    "    seqs = lstm_tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# BERT tokenizer (Huggingface)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 7. Embedding matrix (use GloVe or similar)\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# Load GloVe embeddings or similar (replace with your method)\n",
    "embeddings_index = {}\n",
    "with open('./glove.6B/glove.6B.300d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Fill embedding matrix\n",
    "num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# 8. Dataset class\n",
    "class HybridDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        lstm_seq = lstm_tokenize([row['cleaned_text']])[0]\n",
    "        lstm_seq = torch.tensor(lstm_seq, dtype=torch.long)\n",
    "        \n",
    "        bert_enc = bert_tokenizer.encode_plus(\n",
    "            row['cleaned_text'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = bert_enc['input_ids'].squeeze(0)\n",
    "        attention_mask = bert_enc['attention_mask'].squeeze(0)\n",
    "        \n",
    "        subjectivity = torch.tensor([row['subjectivity']], dtype=torch.float)\n",
    "        identity_flag = torch.tensor([row['identity_flag']], dtype=torch.float)\n",
    "        label = torch.tensor(row['class'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'lstm_seq': lstm_seq,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'subjectivity': subjectivity,\n",
    "            'identity_flag': identity_flag,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# 9. Train/validation/test split and loaders (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "train_df, temp_df = train_test_split(\n",
    "    balanced_df, test_size=0.2, random_state=42, stratify=balanced_df['class']\n",
    ")\n",
    "\n",
    "# Second split: 10% val, 10% test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['class']\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HybridDataset(train_df)\n",
    "val_dataset = HybridDataset(val_df)\n",
    "test_dataset = HybridDataset(test_df)\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "# 10. Hybrid Model Definition (using Keras LSTM structure)\n",
    "class HybridLSTM_BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_matrix, embed_dim=300, lstm_hidden=256, bert_model_name='bert-base-uncased', num_classes=2, dropout=0.3, recurrent_dropout=0.3, l2_reg=0.01):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Fine-tune embeddings\n",
    "\n",
    "        # LSTM layer (implement dropout manually for recurrent connections)\n",
    "        self.lstm_input_dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # BERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Classifier\n",
    "        combined_dim = lstm_hidden + self.bert.config.hidden_size + 2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(combined_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax activation\n",
    "\n",
    "    def forward(self, lstm_seq, input_ids, attention_mask, subjectivity, identity_flag):\n",
    "        # LSTM branch\n",
    "        emb = self.embedding(lstm_seq)  # (batch, seq_len, embed_dim)\n",
    "        emb = self.lstm_input_dropout(emb)\n",
    "        lstm_out, _ = self.lstm(emb)   # (batch, seq_len, lstm_hidden)\n",
    "\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the last output\n",
    "\n",
    "        # BERT branch\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "\n",
    "        # Concatenate\n",
    "        combined = torch.cat([lstm_out, bert_out, subjectivity, identity_flag], dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        x = self.dropout(combined)\n",
    "        logits = self.classifier(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# 11. Training Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = HybridLSTM_BERT(\n",
    "    vocab_size=num_words,\n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.02)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.2,\n",
    "    patience=1,  # Reduce LR after 1 epoch of no val loss improvement\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "epochs = 10  # Allow up to 10 epochs\n",
    "\n",
    "# EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, verbose=False, delta=0.0, path='best_hybrid_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, preds, labels):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved: {self.best_loss:.4f} -> {val_loss:.4f}\")\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            return preds, labels\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return None, None\n",
    "\n",
    "# Initialize early stopper\n",
    "early_stopper = EarlyStopping(patience=2, verbose=True, delta=0.001)\n",
    "\n",
    "\n",
    "# 12. Training Loop with tracking for plotting\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        lstm_seq = batch['lstm_seq'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        subjectivity = batch['subjectivity'].to(device)\n",
    "        identity_flag = batch['identity_flag'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            lstm_seq = batch['lstm_seq'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            subjectivity = batch['subjectivity'].to(device)\n",
    "            identity_flag = batch['identity_flag'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            probs = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping logic\n",
    "    best_preds_temp, best_labels_temp = early_stopper(avg_val_loss, model, all_preds, all_labels)\n",
    "\n",
    "    if best_preds_temp is not None:\n",
    "        best_preds = best_preds_temp\n",
    "        best_labels = best_labels_temp\n",
    "\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# 13. Plotting function\n",
    "def plot_training_history_and_confusion_matrix(train_losses, val_losses, train_acc, val_acc, true_labels, pred_labels):\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss', marker='o')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_losses, label='Training Accuracy', marker='o')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Accuracy', marker='o')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Toxic', 'Toxic'], yticklabels=['Non-Toxic', 'Toxic'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix on Validation Set')\n",
    "    plt.show()\n",
    "\n",
    "# 14. Load Best Model & Prediction function\n",
    "model.load_state_dict(torch.load('best_hybrid_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "def predict(text):\n",
    "    cleaned = clean_text(text)\n",
    "    lstm_seq = lstm_tokenize([cleaned])\n",
    "    lstm_seq = torch.tensor(lstm_seq, dtype=torch.long).to(device)\n",
    "    \n",
    "    bert_enc = bert_tokenizer.encode_plus(\n",
    "        cleaned,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = bert_enc['input_ids'].to(device)\n",
    "    attention_mask = bert_enc['attention_mask'].to(device)\n",
    "    \n",
    "    subjectivity = torch.tensor([[compute_subjectivity(text)]], dtype=torch.float).to(device)\n",
    "    identity_flag = torch.tensor([[has_identity_term(text)]], dtype=torch.float).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "        probs = torch.softmax(probs, dim=1).cpu().numpy()[0]\n",
    "    return {'non_toxic_prob': probs[0], 'toxic_prob': probs[1]}\n",
    "\n",
    "# 15. Calculate and print metrics **after** loading the best model\n",
    "print(\"Calculating and printing final performance metrics...\")\n",
    "\n",
    "# Load best model for evaluation\n",
    "model.load_state_dict(torch.load('best_hybrid_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Perform predictions on the validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        lstm_seq = batch['lstm_seq'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        subjectivity = batch['subjectivity'].to(device)\n",
    "        identity_flag = batch['identity_flag'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        probs = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=['Non-Toxic', 'Toxic'], digits=4))\n",
    "\n",
    "# 16. Plot training history and confusion matrix\n",
    "plot_training_history_and_confusion_matrix(train_losses, val_losses, train_accuracies, val_accuracies, all_labels, all_preds)\n",
    "\n",
    "# 17. Example predictions\n",
    "print(predict(\"You are an idiot!\"))\n",
    "print(predict(\"I love everyone regardless of their background.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3177fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training and loading the best model\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        lstm_seq = batch['lstm_seq'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        subjectivity = batch['subjectivity'].to(device)\n",
    "        identity_flag = batch['identity_flag'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        probs = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Save for use in another cell\n",
    "import numpy as np\n",
    "np.save('all_preds.npy', np.array(all_preds))\n",
    "np.save('all_labels.npy', np.array(all_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abb60bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9489\n",
      "Precision: 0.9531\n",
      "Recall:    0.9443\n",
      "F1 Score:  0.9487\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load predictions and labels\n",
    "all_preds = np.load('all_preds.npy')\n",
    "all_labels = np.load('all_labels.npy')\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='binary')\n",
    "recall = recall_score(all_labels, all_preds, average='binary')\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
