{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging datasets...\n",
      "Merged dataset shape: (184354, 2)\n",
      "\n",
      "Balancing classes...\n",
      "Balanced dataset shape: (71828, 2)\n",
      "\n",
      "Cleaning text...\n",
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 2170, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\ultratb.py\", line 1182, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\executing\\executing.py\", line 264, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\executing\\executing.py\", line 183, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\executing\\executing.py\", line 212, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, tuple(lines))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\executing\\executing.py\", line 223, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Angelika Vergara\\AppData\\Roaming\\Python\\Python312\\site-packages\\executing\\executing.py\", line 163, in __init__\n",
      "    self.tree = ast.parse(self.text, filename=filename)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Angelika Vergara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ast.py\", line 52, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 1. Prepare Labels function\n",
    "def prepare_labels(df):\n",
    "    text_col = None\n",
    "    for possible_col in [\"comment_text\", \"text\"]:\n",
    "        if possible_col in df.columns:\n",
    "            text_col = possible_col\n",
    "            break\n",
    "    if text_col is None:\n",
    "        raise ValueError(f\"No recognized text column found. Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    if \"toxic\" in df.columns:\n",
    "        df[\"class\"] = np.where(df[\"toxic\"] >= 0.5, 1, 0)\n",
    "    else:\n",
    "        raise ValueError(f\"No 'toxic' column found for labels. Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    return df[[text_col, \"class\"]].rename(columns={text_col: \"tweet\"})\n",
    "\n",
    "# 2. Load and merge datasets\n",
    "print(\"Loading and merging datasets...\")\n",
    "\n",
    "df1 = pd.read_csv(\"./data/labeled_data.csv\")  # Davidson et al.\n",
    "df1 = df1.rename(columns={\"tweet\": \"tweet\", \"class\": \"label\"})\n",
    "df1[\"class\"] = df1[\"label\"].apply(lambda x: 1 if x in [0, 1] else 0)\n",
    "df1 = df1[[\"tweet\", \"class\"]]\n",
    "\n",
    "df2 = pd.read_csv(\"./data/train.csv\")  # Jigsaw dataset\n",
    "df2_prepared = prepare_labels(df2)\n",
    "\n",
    "combined_df = pd.concat([df1, df2_prepared], ignore_index=True)\n",
    "combined_df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "\n",
    "print(\"Merged dataset shape:\", combined_df.shape)\n",
    "prepared_df = combined_df\n",
    "\n",
    "# 3. Balance dataset\n",
    "print(\"\\nBalancing classes...\")\n",
    "df_majority = prepared_df[prepared_df[\"class\"] == 0]\n",
    "df_minority = prepared_df[prepared_df[\"class\"] == 1]\n",
    "if len(df_minority) == 0:\n",
    "    raise ValueError(\"No toxic samples found in dataset!\")\n",
    "\n",
    "df_majority_downsampled = resample(df_majority, n_samples=len(df_minority), random_state=42)\n",
    "balanced_df = pd.concat([df_majority_downsampled, df_minority]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced dataset shape:\", balanced_df.shape)\n",
    "\n",
    "# 4. Text cleaning utils\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|\\#\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"\\nCleaning text...\")\n",
    "balanced_df['cleaned_text'] = balanced_df['tweet'].apply(clean_text)\n",
    "\n",
    "# 5. Subjectivity and identity term features\n",
    "IDENTITY_TERMS = [\n",
    "    \"muslim\", \"jew\", \"black\", \"white\", \"asian\", \"latino\", \"women\", \"men\", \"gay\", \"lesbian\",\n",
    "    \"trans\", \"christian\", \"atheist\", \"immigrant\", \"indian\", \"arab\", \"african\", \"hispanic\"\n",
    "]\n",
    "\n",
    "def compute_subjectivity(text):\n",
    "    return float(TextBlob(text).sentiment.subjectivity)\n",
    "\n",
    "def has_identity_term(text):\n",
    "    text_lower = text.lower()\n",
    "    return int(any(term in text_lower for term in IDENTITY_TERMS))\n",
    "\n",
    "balanced_df['subjectivity'] = balanced_df['tweet'].apply(compute_subjectivity)\n",
    "balanced_df['identity_flag'] = balanced_df['tweet'].apply(has_identity_term)\n",
    "\n",
    "# 6. Tokenizer setup\n",
    "MAX_LEN = 128\n",
    "VOCAB_SIZE = 30000  # Consistent with your embedding layer\n",
    "\n",
    "# LSTM tokenizer (Keras style)\n",
    "lstm_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "lstm_tokenizer.fit_on_texts(balanced_df['cleaned_text'].tolist())\n",
    "word_index = lstm_tokenizer.word_index\n",
    "\n",
    "def lstm_tokenize(texts):\n",
    "    seqs = lstm_tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# BERT tokenizer (Huggingface)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 7. Embedding matrix (use GloVe or similar)\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# Load GloVe embeddings or similar (replace with your method)\n",
    "embeddings_index = {}\n",
    "with open('./glove.6B/glove.6B.300d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Fill embedding matrix\n",
    "num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# 8. Dataset class\n",
    "class HybridDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        lstm_seq = lstm_tokenize([row['cleaned_text']])[0]\n",
    "        lstm_seq = torch.tensor(lstm_seq, dtype=torch.long)\n",
    "        \n",
    "        bert_enc = bert_tokenizer.encode_plus(\n",
    "            row['cleaned_text'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = bert_enc['input_ids'].squeeze(0)\n",
    "        attention_mask = bert_enc['attention_mask'].squeeze(0)\n",
    "        \n",
    "        subjectivity = torch.tensor([row['subjectivity']], dtype=torch.float)\n",
    "        identity_flag = torch.tensor([row['identity_flag']], dtype=torch.float)\n",
    "        label = torch.tensor(row['class'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'lstm_seq': lstm_seq,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'subjectivity': subjectivity,\n",
    "            'identity_flag': identity_flag,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# 9. Train/validation split and loaders\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.1, random_state=42, stratify=balanced_df['class'])\n",
    "\n",
    "train_dataset = HybridDataset(train_df)\n",
    "val_dataset = HybridDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# 10. Hybrid Model Definition (using Keras LSTM structure)\n",
    "class HybridLSTM_BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_matrix, embed_dim=300, lstm_hidden=256, bert_model_name='bert-base-uncased', num_classes=2, dropout=0.3, recurrent_dropout=0.3, l2_reg=0.01):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Fine-tune embeddings\n",
    "\n",
    "        # LSTM layer (implement dropout manually for recurrent connections)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # BERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Classifier\n",
    "        combined_dim = lstm_hidden + self.bert.config.hidden_size + 2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(combined_dim, num_classes)\n",
    "\n",
    "    def forward(self, lstm_seq, input_ids, attention_mask, subjectivity, identity_flag):\n",
    "        # LSTM branch\n",
    "        emb = self.embedding(lstm_seq)  # (batch, seq_len, embed_dim)\n",
    "        lstm_out, _ = self.lstm(emb)   # (batch, seq_len, lstm_hidden)\n",
    "\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the last output\n",
    "\n",
    "        # BERT branch\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "\n",
    "        # Concatenate\n",
    "        combined = torch.cat([lstm_out, bert_out, subjectivity, identity_flag], dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        x = self.dropout(combined)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# 11. Training Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HybridLSTM_BERT(\n",
    "    vocab_size=num_words,\n",
    "    embedding_matrix=embedding_matrix\n",
    ").to(device)  # Pass num_words\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01) # Keep some L2\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2, verbose=True) # Learning rate decay\n",
    "\n",
    "epochs = 50\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "# 12. Training Loop with tracking for plotting\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        lstm_seq = batch['lstm_seq'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        subjectivity = batch['subjectivity'].to(device)\n",
    "        identity_flag = batch['identity_flag'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            lstm_seq = batch['lstm_seq'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            subjectivity = batch['subjectivity'].to(device)\n",
    "            identity_flag = batch['identity_flag'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss) # Learning rate decay\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_hybrid_model.pt')\n",
    "        patience_counter = 0\n",
    "        best_preds = all_preds\n",
    "        best_labels = all_labels\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 13. Plotting function\n",
    "def plot_training_history_and_confusion_matrix(train_losses, val_losses, train_acc, val_acc, true_labels, pred_labels):\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss', marker='o')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_acc, label='Training Accuracy', marker='o')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='o')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Toxic', 'Toxic'], yticklabels=['Non-Toxic', 'Toxic'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix on Validation Set')\n",
    "    plt.show()\n",
    "\n",
    "# 14. Load Best Model & Prediction function\n",
    "model.load_state_dict(torch.load('best_hybrid_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "def predict(text):\n",
    "    cleaned = clean_text(text)\n",
    "    lstm_seq = lstm_tokenize([cleaned])\n",
    "    lstm_seq = torch.tensor(lstm_seq, dtype=torch.long).to(device)\n",
    "    \n",
    "    bert_enc = bert_tokenizer.encode_plus(\n",
    "        cleaned,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = bert_enc['input_ids'].to(device)\n",
    "    attention_mask = bert_enc['attention_mask'].to(device)\n",
    "    \n",
    "    subjectivity = torch.tensor([[compute_subjectivity(text)]], dtype=torch.float).to(device)\n",
    "    identity_flag = torch.tensor([[has_identity_term(text)]], dtype=torch.float).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(lstm_seq, input_ids, attention_mask, subjectivity, identity_flag)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    return {'non_toxic_prob': probs[0], 'toxic_prob': probs[1]}\n",
    "\n",
    "# 15. Plot training history and confusion matrix\n",
    "plot_training_history_and_confusion_matrix(train_losses, val_losses, train_accuracies, val_accuracies, best_labels, best_preds)\n",
    "\n",
    "# 16. Example predictions\n",
    "print(predict(\"You are an idiot!\"))\n",
    "print(predict(\"I love everyone regardless of their background.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
